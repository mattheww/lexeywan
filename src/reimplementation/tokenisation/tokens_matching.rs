//! Applies the Pest tokenisation grammar and represents data from the resulting matches.
//!
//! See tokenise.pest in this directory for the grammar itself.
//!
//! All Pest-specific code is isolated to this module, other than the Nonterminal enumeration.

use crate::Edition;
use crate::reimplementation::pegs::{
    MatchData, Multiplicity, Outcome, WrittenUp, attempt_pest_match, extract_only_item,
};

/// Matches as much as possible using the specified edition's tokens nonterminal.
///
/// Reports an error message if it finds a problem in lex_via_peg's model or implementation
/// (in particular, if the match attempt fails).
pub fn match_tokens(edition: Edition, input: &[char]) -> Result<TokensMatchData, String> {
    use Multiplicity::*;
    let (tokens_rule, token_rule) = token_rules_for_edition(edition);
    let s: String = input.iter().collect();

    let Outcome::Success {
        pair: tokens_pair,
        consumed_entire_input,
    } = attempt_pest_match::<Nonterminal, TokenParser>(tokens_rule, &s)?
    else {
        return Err("Pest reported no match of the tokens rule".to_owned());
    };

    let mut token_kind_matches = Vec::new();
    for token_pair in tokens_pair.into_inner() {
        if token_pair.as_rule() != token_rule {
            return Err(format!(
                "Pest matched {:?} under the tokens rule",
                token_pair.as_rule()
            ));
        }
        let token_kind_pair = extract_only_item(token_pair.into_inner()).map_err(|m| match m {
            NoItems => "Pest reported empty match of the token rule".to_owned(),
            Multiple => "Pest reported multiple tokens under the token rule".to_owned(),
        })?;
        token_kind_matches.push(TokenKindMatch::new(token_kind_pair));
    }
    Ok(TokensMatchData {
        token_kind_matches,
        consumed_entire_input,
    })
}

/// Information about an attempt to match an edition's tokens nonterminal.
///
/// The tokens nonterminal's expression is a zero-or-more repetitions expression, so the match
/// attempt is always successful.
pub struct TokensMatchData {
    /// Each sub-match of a token-kind nonterminal
    pub token_kind_matches: Vec<TokenKindMatch>,
    /// Whether the edition's tokens nonterminal consumed all the input
    pub consumed_entire_input: bool,
}

#[derive(pest_derive::Parser)]
#[grammar = "reimplementation/tokenisation/tokenise.pest"]
/// Parser for the tokenisation grammar, generated by Pest.
struct TokenParser;

/// Enumeration of the nonterminals used in the tokenisation grammar.
///
/// This includes:
/// - the tokens nonterminals     (named like TOKENS_yyyy)
/// - the token nonterminals      (named like TOKEN_yyyy)
/// - the token-kind nonterminals (named in Title_case)
/// - subsidiary nonterminals     (named in UPPER_CASE)
///
/// Some members are nonterminals in the Pest grammar but documented as terminals in the writeup;
/// see the is_documented_as_terminal implementation below.
pub type Nonterminal = Rule;

/// Returns the Pest TOKENS and TOKEN rules to use for the specified Rust edition.
fn token_rules_for_edition(edition: Edition) -> (Rule, Rule) {
    match edition {
        Edition::E2015 => (Nonterminal::TOKENS_2015, Nonterminal::TOKEN_2015),
        Edition::E2021 => (Nonterminal::TOKENS_2021, Nonterminal::TOKEN_2021),
        Edition::E2024 => (Nonterminal::TOKENS_2024, Nonterminal::TOKEN_2024),
    }
}

/// Information from a successful match attempt of a token-kind nonterminal.
///
/// As far as the type system is concerned this could be a match of any nonterminal from the
/// tokenisation grammar, but we only use it for token-kind nonterminals.
pub type TokenKindMatch = MatchData<Nonterminal>;

impl WrittenUp for Nonterminal {
    fn is_documented_as_terminal(&self) -> bool {
        // TAB is also documented as a terminal, but it only appears in the frontmatter grammar.
        *self == Nonterminal::DOUBLEQUOTE
            || *self == Nonterminal::BACKSLASH
            || *self == Nonterminal::LF
    }
}
